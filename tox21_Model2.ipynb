{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tox21_Model2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "tyo9XGhyQnup",
        "DcgVBK1gM-DC"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V49aUWwS3zmZ",
        "colab_type": "text"
      },
      "source": [
        "# Preparación de la Notebook\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-vdXM4u0Cv6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "93f0b3ab-4a02-4f9c-e79b-0dd30e42b7f4"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append('/usr/local/lib/python3.7/site-packages/')\n",
        "\n",
        "#!apt install  openbabel\n",
        "\n",
        "# import rdkit\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# from rdkit.Chem import PandasTools\n",
        "\n",
        "!pip install git+https://github.com/autonomio/talos@1.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/autonomio/talos@1.0\n",
            "  Cloning https://github.com/autonomio/talos (to revision 1.0) to /tmp/pip-req-build-df3jxa38\n",
            "  Running command git clone -q https://github.com/autonomio/talos /tmp/pip-req-build-df3jxa38\n",
            "  Running command git checkout -b 1.0 --track origin/1.0\n",
            "  Switched to a new branch '1.0'\n",
            "  Branch '1.0' set up to track remote branch '1.0' from 'origin'.\n",
            "Requirement already satisfied: tensorflow>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from talos==1.0) (2.2.0)\n",
            "Collecting statsmodels>=0.11.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cb/83/540fd83238a18abe6c2d280fa8e489ac5fcefa1f370f0ca1acd16ae1b860/statsmodels-0.11.1-cp36-cp36m-manylinux1_x86_64.whl (8.7MB)\n",
            "\u001b[K     |████████████████████████████████| 8.7MB 8.3MB/s \n",
            "\u001b[?25hCollecting wrangle\n",
            "  Downloading https://files.pythonhosted.org/packages/85/35/bc729e377417613f2d062a890faea5d649ef1a554df21499e9c3a4a5501a/wrangle-0.6.7.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from talos==1.0) (1.18.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from talos==1.0) (1.0.5)\n",
            "Collecting astetik\n",
            "  Downloading https://files.pythonhosted.org/packages/3c/ba/f8622951da73d9b47b45bb847112c388651f9c6e413e712954f260301d9f/astetik-1.9.9.tar.gz\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from talos==1.0) (0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from talos==1.0) (4.41.1)\n",
            "Collecting chances\n",
            "  Downloading https://files.pythonhosted.org/packages/fa/d8/d61112d7476dc3074b855f1edd8556cde9b49b7106853f0b060109dd4c82/chances-0.1.9.tar.gz\n",
            "Collecting kerasplotlib\n",
            "  Downloading https://files.pythonhosted.org/packages/7b/b7/31663d3b5ea9afd8c2c6ffa06d3c4e118ef363e12dc75b7c49fb6a2d22aa/kerasplotlib-0.1.6.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from talos==1.0) (2.23.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->talos==1.0) (3.12.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->talos==1.0) (3.2.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->talos==1.0) (1.15.0)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->talos==1.0) (2.2.2)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->talos==1.0) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->talos==1.0) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->talos==1.0) (2.2.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->talos==1.0) (1.6.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->talos==1.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->talos==1.0) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->talos==1.0) (0.9.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->talos==1.0) (0.34.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->talos==1.0) (1.30.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->talos==1.0) (0.3.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->talos==1.0) (0.2.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->talos==1.0) (2.10.0)\n",
            "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.6/dist-packages (from statsmodels>=0.11.0->talos==1.0) (0.5.1)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from wrangle->talos==1.0) (2.3.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->talos==1.0) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->talos==1.0) (2.8.1)\n",
            "Collecting geonamescache\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/ba/b7939087621bfeb24c0f52c4b879865a9f902cda72efd119f4275400e692/geonamescache-1.2.0-py3-none-any.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 47.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->talos==1.0) (0.22.2.post1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from kerasplotlib->talos==1.0) (5.5.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->talos==1.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->talos==1.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->talos==1.0) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->talos==1.0) (3.0.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow>=2.0.0->talos==1.0) (49.1.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.0.0->talos==1.0) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.0.0->talos==1.0) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.0.0->talos==1.0) (1.17.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.0.0->talos==1.0) (3.2.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.0.0->talos==1.0) (1.7.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->wrangle->talos==1.0) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->wrangle->talos==1.0) (1.0.8)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->talos==1.0) (0.16.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->kerasplotlib->talos==1.0) (4.3.3)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->kerasplotlib->talos==1.0) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->kerasplotlib->talos==1.0) (2.1.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->kerasplotlib->talos==1.0) (4.4.2)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->kerasplotlib->talos==1.0) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->kerasplotlib->talos==1.0) (1.0.18)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->kerasplotlib->talos==1.0) (0.8.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.0.0->talos==1.0) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.0.0->talos==1.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.0.0->talos==1.0) (4.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.0.0->talos==1.0) (4.6)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.0.0->talos==1.0) (1.7.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->kerasplotlib->talos==1.0) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->kerasplotlib->talos==1.0) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->kerasplotlib->talos==1.0) (0.2.5)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.0.0->talos==1.0) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.0.0->talos==1.0) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.0.0->talos==1.0) (3.1.0)\n",
            "Building wheels for collected packages: talos, wrangle, astetik, chances, kerasplotlib\n",
            "  Building wheel for talos (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for talos: filename=talos-1.0-cp36-none-any.whl size=53700 sha256=708ac26cdef0d75efe07eadacb56e326eecb406bf30a101cbb6810af812a70d0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9v8qv31v/wheels/9f/14/d1/e58012a97c43cf2148959890b171f49e4a5e1c82a9946b2c22\n",
            "  Building wheel for wrangle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrangle: filename=wrangle-0.6.7-cp36-none-any.whl size=49894 sha256=805e43d780f014ce0b3db25e5263c4fcb5332bd954706f731f32fbe483886fe1\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/1b/50/d0403ce6ef269e364894da7b50db68db14c4ac62c577561e2d\n",
            "  Building wheel for astetik (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for astetik: filename=astetik-1.9.9-cp36-none-any.whl size=56960 sha256=2ec59e2dadb73e7e3fedf420d61f38a728c667bff1987b96c7f4931f02a058db\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/70/21/c475cd079ec401dd6e1b9b1d42b4c38554ce12679bfb214aad\n",
            "  Building wheel for chances (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chances: filename=chances-0.1.9-cp36-none-any.whl size=41609 sha256=66ed46214ad6d871313fc453f93193349211a490d8cbc9bc04af1b70100a2665\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/33/46/c871b94249bd57d17797d049b3dff8e3a09c315afb67eb14c6\n",
            "  Building wheel for kerasplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kerasplotlib: filename=kerasplotlib-0.1.6-cp36-none-any.whl size=3601 sha256=cbfa3c1f1a8963f73893ed1973bd6cc7b366184cf39259c3812b80cd25490082\n",
            "  Stored in directory: /root/.cache/pip/wheels/9d/d3/8c/9503a22b0a38e8b21c70ad834e4606d209193443e5c709305d\n",
            "Successfully built talos wrangle astetik chances kerasplotlib\n",
            "\u001b[31mERROR: wrangle 0.6.7 has requirement scipy==1.2, but you'll have scipy 1.4.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: statsmodels, wrangle, geonamescache, astetik, chances, kerasplotlib, talos\n",
            "  Found existing installation: statsmodels 0.10.2\n",
            "    Uninstalling statsmodels-0.10.2:\n",
            "      Successfully uninstalled statsmodels-0.10.2\n",
            "Successfully installed astetik-1.9.9 chances-0.1.9 geonamescache-1.2.0 kerasplotlib-0.1.6 statsmodels-0.11.1 talos-1.0 wrangle-0.6.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRlOCTBGcoRC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "outputId": "87ec310c-0418-40fa-99b1-2e69bd7dda23"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')\n",
        "%cd drive/My Drive/finalRedes\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "/content/drive/.shortcut-targets-by-id/1vnOFBVFFehrvQ_ZlIpZVR7M-VhQvq5p-/finalRedes\n",
            "bestmodel.hdf5\t\t\t   tox21_SCAN4_activations_firstNeuron\n",
            "Descriptores_nr_ar.csv\t\t   tox21_SCAN4_activations_initializers\n",
            "Descriptores_nr_er.csv\t\t   tox21_SCAN4_activations_initializers2\n",
            "Descriptores_test.csv\t\t   tox21_SCAN4_relu_dropANDhiddenLayers\n",
            "Descriptores_test_new.csv\t   tox21_SCAN4_relu_dropout\n",
            "logs\t\t\t\t   tox21_SCAN4_relu_dropout_bathcNorm\n",
            "Miniconda3-latest-Linux-x86_64.sh  tox21_SCAN4_relu_firstNeuron\n",
            "nr_ar_activity.txt\t\t   tox21_SCAN4_relu_kernelReg\n",
            "nr-ar.sdf\t\t\t   tox21_SCAN4_relu_kernelReg_accuracy\n",
            "nr_ar_test_activity.txt\t\t   tox21_SCAN4_relu_shapes\n",
            "nr_er_activity.txt\t\t   tox21_SCAN_nr_er_class_weight\n",
            "nr-er.sdf\t\t\t   tox21_SCAN_nr_er_class_weight2\n",
            "nr_er_test_activity.txt\t\t   tox21_SCAN_nr_er_dropout_activations\n",
            "nr_er_test_new_activity.txt\t   tox21_SCAN_nr_er_fbeta2_1\n",
            "tox21_SCAN1\t\t\t   tox21_SCAN_nr_er_first_neuron_shape\n",
            "tox21_SCAN2\t\t\t   tox21_SCAN_nr_er_FNeu_DO\n",
            "tox21_SCAN3\t\t\t   tox21_SCAN_nr_er_meui\n",
            "tox21_SCAN4_activations\t\t   tox21_SCAN_nr_er_regularizacion\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOzbHfqP4ChN",
        "colab_type": "text"
      },
      "source": [
        "# Preparación de los datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwM2ExRQ4YiT",
        "colab_type": "text"
      },
      "source": [
        "Archivo con Descriptores calculados para el efecto toxico NR-ER."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTgQah_nh_ro",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "outputId": "7191b0c8-d693-47e7-fd04-b51f4b185d73"
      },
      "source": [
        "desc_path = 'Descriptores_nr_er.csv' #Este archivo se obtuvo con la notebook tox21_DescriptorsCalc\n",
        "descriptores = pd.read_csv(desc_path)\n",
        "descriptores"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>abonds</th>\n",
              "      <th>atoms</th>\n",
              "      <th>bonds</th>\n",
              "      <th>cansmi</th>\n",
              "      <th>cansmiNS</th>\n",
              "      <th>dbonds</th>\n",
              "      <th>formula</th>\n",
              "      <th>HBA1</th>\n",
              "      <th>HBA2</th>\n",
              "      <th>HBD</th>\n",
              "      <th>InChI</th>\n",
              "      <th>InChIKey</th>\n",
              "      <th>L5</th>\n",
              "      <th>logP</th>\n",
              "      <th>MP</th>\n",
              "      <th>MR</th>\n",
              "      <th>MW</th>\n",
              "      <th>nF</th>\n",
              "      <th>rotors</th>\n",
              "      <th>s</th>\n",
              "      <th>sbonds</th>\n",
              "      <th>smarts</th>\n",
              "      <th>tbonds</th>\n",
              "      <th>title</th>\n",
              "      <th>TPSA</th>\n",
              "      <th>CHARGE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.7767</td>\n",
              "      <td>133.0267</td>\n",
              "      <td>86.5125</td>\n",
              "      <td>378.312159</td>\n",
              "      <td>6.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.15</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>18.0</td>\n",
              "      <td>69.0</td>\n",
              "      <td>73.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.1944</td>\n",
              "      <td>139.7661</td>\n",
              "      <td>140.9350</td>\n",
              "      <td>457.603800</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>55.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.93</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>17.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.5770</td>\n",
              "      <td>298.4234</td>\n",
              "      <td>73.7772</td>\n",
              "      <td>253.262640</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>129.62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>12.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>61.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0394</td>\n",
              "      <td>202.9346</td>\n",
              "      <td>133.0344</td>\n",
              "      <td>494.475279</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>48.78</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>91.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.9826</td>\n",
              "      <td>8.9656</td>\n",
              "      <td>145.4004</td>\n",
              "      <td>474.855700</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>91.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.69</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7692</th>\n",
              "      <td>7692</td>\n",
              "      <td>6.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.3850</td>\n",
              "      <td>179.0045</td>\n",
              "      <td>46.8274</td>\n",
              "      <td>170.232100</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>80.74</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7693</th>\n",
              "      <td>7693</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1217</td>\n",
              "      <td>82.7273</td>\n",
              "      <td>35.6454</td>\n",
              "      <td>102.158140</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>56.15</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7694</th>\n",
              "      <td>7694</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1217</td>\n",
              "      <td>82.7273</td>\n",
              "      <td>35.6454</td>\n",
              "      <td>102.158140</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>56.15</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7695</th>\n",
              "      <td>7695</td>\n",
              "      <td>6.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.4449</td>\n",
              "      <td>42.3956</td>\n",
              "      <td>73.9150</td>\n",
              "      <td>291.260621</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>115.41</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7696</th>\n",
              "      <td>7696</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.9721</td>\n",
              "      <td>84.2486</td>\n",
              "      <td>101.6010</td>\n",
              "      <td>398.558340</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>46.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>195.30</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7697 rows × 27 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0  abonds  atoms  bonds  ...  tbonds  title    TPSA  CHARGE\n",
              "0              0    11.0   42.0   44.0  ...     0.0    0.0   45.15       0\n",
              "1              1    18.0   69.0   73.0  ...     0.0    0.0   30.93       0\n",
              "2              2    17.0   30.0   32.0  ...     0.0    0.0  129.62       0\n",
              "3              3    12.0   59.0   61.0  ...     0.0    0.0   48.78       0\n",
              "4              4     0.0   92.0   91.0  ...     0.0    0.0   27.69       1\n",
              "...          ...     ...    ...    ...  ...     ...    ...     ...     ...\n",
              "7692        7692     6.0   21.0   21.0  ...     0.0    0.0   80.74       0\n",
              "7693        7693     0.0   12.0   12.0  ...     0.0    0.0   56.15       0\n",
              "7694        7694     0.0   12.0   12.0  ...     0.0    0.0   56.15       0\n",
              "7695        7695     6.0   32.0   32.0  ...     0.0    0.0  115.41       0\n",
              "7696        7696     0.0   50.0   49.0  ...     0.0    0.0  195.30       0\n",
              "\n",
              "[7697 rows x 27 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJ8DL0lypBbm",
        "colab_type": "text"
      },
      "source": [
        "Obtenemos el vector de clases segun las actividades\n",
        "\n",
        "0 : inactivo\n",
        "\n",
        "1 : activo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zSsSNzAkB57",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0b7cd329-6b14-4659-dd16-c9ffad1a13b9"
      },
      "source": [
        "#Read target.\n",
        "actives_f = open(\"nr_er_activity.txt\", \"r\") #Este archivo se obtuvo de la columna de Actividad del SDF.\n",
        "activity = []\n",
        "for x in actives_f:\n",
        "  activity.append(x[0])\n",
        "#activity = activity[1:]\n",
        "activity = [int(i) for i in activity]\n",
        "y = np.array(activity)\n",
        "print(y.shape)\n",
        "actives_f.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7697,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Q3yDN2ApOO2",
        "colab_type": "text"
      },
      "source": [
        "Vemos la distribucion de Activos e Inactivos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9_vzXqDSx-5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "59860443-b3a9-4cec-e892-7c1988aaadb3"
      },
      "source": [
        "#Visualizar activos vs inactivos: \n",
        "activos = sum(y)\n",
        "inactivos = sum((y == 0)*1)\n",
        "indices = ['Activos','Inactivos']\n",
        "print('Proporcion de activos en el dataset:', activos/(activos+inactivos))\n",
        "activity_df = pd.DataFrame(data = [activos, inactivos], columns = ['NR-ER'], index = indices)\n",
        "activity_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Proporcion de activos en el dataset: 0.1217357411978693\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NR-ER</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Activos</th>\n",
              "      <td>937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Inactivos</th>\n",
              "      <td>6760</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           NR-ER\n",
              "Activos      937\n",
              "Inactivos   6760"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6H4Zs-O4hrr",
        "colab_type": "text"
      },
      "source": [
        "Train - Validation split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR3cKhIDjuay",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "439f2b7a-bdad-4b27-cc08-8afea7699171"
      },
      "source": [
        "#Train-Val Split. \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(descriptores.values, y, test_size =0.1,random_state = 42, stratify=activity)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_train.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6927, 27)\n",
            "(770, 27)\n",
            "(6927,)\n",
            "(770,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU6iWVEc-mjg",
        "colab_type": "text"
      },
      "source": [
        "# Procesamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6Qrx06K-txz",
        "colab_type": "text"
      },
      "source": [
        "Normalización"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ws3FfOEj2ud",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "74b7e968-073e-4b40-92e7-4f255ef1ef98"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "desc_scaler = StandardScaler()\n",
        "X_train = desc_scaler.fit_transform(X_train)\n",
        "Scaled_descriptors = pd.DataFrame(X_train, columns=descriptores.columns)\n",
        "Scaled_descriptors.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>abonds</th>\n",
              "      <th>atoms</th>\n",
              "      <th>bonds</th>\n",
              "      <th>cansmi</th>\n",
              "      <th>cansmiNS</th>\n",
              "      <th>dbonds</th>\n",
              "      <th>formula</th>\n",
              "      <th>HBA1</th>\n",
              "      <th>HBA2</th>\n",
              "      <th>HBD</th>\n",
              "      <th>InChI</th>\n",
              "      <th>InChIKey</th>\n",
              "      <th>L5</th>\n",
              "      <th>logP</th>\n",
              "      <th>MP</th>\n",
              "      <th>MR</th>\n",
              "      <th>MW</th>\n",
              "      <th>nF</th>\n",
              "      <th>rotors</th>\n",
              "      <th>s</th>\n",
              "      <th>sbonds</th>\n",
              "      <th>smarts</th>\n",
              "      <th>tbonds</th>\n",
              "      <th>title</th>\n",
              "      <th>TPSA</th>\n",
              "      <th>CHARGE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.867422</td>\n",
              "      <td>0.061204</td>\n",
              "      <td>0.681038</td>\n",
              "      <td>0.628320</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.267463</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.776294</td>\n",
              "      <td>-0.191243</td>\n",
              "      <td>-0.088211</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.631796</td>\n",
              "      <td>-0.323901</td>\n",
              "      <td>0.492766</td>\n",
              "      <td>0.159438</td>\n",
              "      <td>-0.148362</td>\n",
              "      <td>0.746655</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.686063</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.168185</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.474732</td>\n",
              "      <td>0.113714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.218618</td>\n",
              "      <td>0.061204</td>\n",
              "      <td>-0.696640</td>\n",
              "      <td>-0.695395</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.890852</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.664650</td>\n",
              "      <td>-0.746224</td>\n",
              "      <td>-0.088211</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.607300</td>\n",
              "      <td>-0.709506</td>\n",
              "      <td>-0.772663</td>\n",
              "      <td>-0.858691</td>\n",
              "      <td>-0.148362</td>\n",
              "      <td>-0.522930</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.697721</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.168185</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.677815</td>\n",
              "      <td>0.113714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.849975</td>\n",
              "      <td>0.061204</td>\n",
              "      <td>-0.554122</td>\n",
              "      <td>-0.558459</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.267463</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.520556</td>\n",
              "      <td>-0.191243</td>\n",
              "      <td>-0.636224</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.466974</td>\n",
              "      <td>-0.482779</td>\n",
              "      <td>-0.583903</td>\n",
              "      <td>-0.568211</td>\n",
              "      <td>-0.148362</td>\n",
              "      <td>-0.311333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.598879</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.168185</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.421236</td>\n",
              "      <td>0.113714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.431886</td>\n",
              "      <td>0.061204</td>\n",
              "      <td>0.015952</td>\n",
              "      <td>-0.010714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.890852</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.127869</td>\n",
              "      <td>-0.191243</td>\n",
              "      <td>1.007816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.138689</td>\n",
              "      <td>-0.095840</td>\n",
              "      <td>-0.162152</td>\n",
              "      <td>-0.283662</td>\n",
              "      <td>-0.148362</td>\n",
              "      <td>0.323460</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.043592</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.168185</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.136819</td>\n",
              "      <td>0.113714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.397894</td>\n",
              "      <td>-0.992992</td>\n",
              "      <td>0.633532</td>\n",
              "      <td>0.719611</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.979316</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.560152</td>\n",
              "      <td>-0.468734</td>\n",
              "      <td>-0.636224</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.756467</td>\n",
              "      <td>0.446433</td>\n",
              "      <td>0.420815</td>\n",
              "      <td>0.225386</td>\n",
              "      <td>-0.148362</td>\n",
              "      <td>-0.946125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.982589</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.168185</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.444546</td>\n",
              "      <td>0.113714</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0    abonds     atoms  ...  title      TPSA    CHARGE\n",
              "0   -0.867422  0.061204  0.681038  ...    0.0 -0.474732  0.113714\n",
              "1   -0.218618  0.061204 -0.696640  ...    0.0 -0.677815  0.113714\n",
              "2    0.849975  0.061204 -0.554122  ...    0.0 -0.421236  0.113714\n",
              "3   -0.431886  0.061204  0.015952  ...    0.0 -0.136819  0.113714\n",
              "4   -1.397894 -0.992992  0.633532  ...    0.0 -0.444546  0.113714\n",
              "\n",
              "[5 rows x 27 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URdOocrrkWM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Validation Scale:\n",
        "X_val = desc_scaler.transform(X_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfhyZoJm-8sx",
        "colab_type": "text"
      },
      "source": [
        "Paquetes de procesamiento\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UOZbOaZkn2q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6b1ff02a-56da-4a7e-ee5f-30ac350388bf"
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras.metrics import AUC, TruePositives, FalsePositives, FalseNegatives\n",
        "from keras.layers import Dense, Flatten, Activation\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import BatchNormalization, PReLU, ELU, LeakyReLU\n",
        "from keras.layers import Conv2D, AveragePooling2D, BatchNormalization, Dropout, MaxPooling2D\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
        "from keras import optimizers, initializers\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from keras.initializers import glorot_uniform, he_uniform, uniform, Constant\n",
        "from keras.constraints import maxnorm\n",
        "from keras.regularizers import l2\n",
        "\n",
        "import talos\n",
        "import datetime\n",
        "from keras.callbacks import TensorBoard\n",
        "from talos.model.network_shape import network_shape\n",
        "from talos.model.normalizers import lr_normalizer\n",
        "from keras.callbacks import EarlyStopping,ReduceLROnPlateau, ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6FqjJTeQWq2",
        "colab_type": "text"
      },
      "source": [
        "## Busqueda de Hiperparametros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrgVv1NJTKOJ",
        "colab_type": "text"
      },
      "source": [
        "Busqueda de Hiperparametros con Talos Scan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2SqpoX5bBTS",
        "colab_type": "text"
      },
      "source": [
        "Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_-dwXyvpZsP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def fbeta(y_true, y_pred, threshold_shift=0.2, beta = 2):\n",
        "\n",
        "    # just in case of hipster activation at the final layer\n",
        "    y_pred = K.clip(y_pred, 0, 1)\n",
        "\n",
        "    # shifting the prediction threshold from .5 if needed\n",
        "    y_pred_bin = K.round(y_pred - threshold_shift)\n",
        "\n",
        "    tp = K.sum(K.round(y_true * y_pred_bin)) + K.epsilon()\n",
        "    fp = K.sum(K.round(K.clip(y_pred_bin - y_true, 0, 1)))\n",
        "    fn = K.sum(K.round(K.clip(y_true - y_pred, 0, 1)))\n",
        "\n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "\n",
        "    beta_squared = beta ** 2\n",
        "    return (beta_squared + 1) * (precision * recall) / (beta_squared * precision + recall + K.epsilon())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6wjJfAxzDQ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def tox21_model(x,y,params): \n",
        "    model = Sequential()\n",
        "\n",
        "    # Input layer\n",
        "    model.add(Dense(params['first_neuron'], input_shape = (X_train.shape[1],), \n",
        "                    kernel_initializer=params['kernel_initializer'],\n",
        "                    activation = params['activation']))\n",
        "    # Input Dropout layer\n",
        "    if(params['dropout']>0):\n",
        "        model.add(Dropout(params['dropout']))\n",
        "\n",
        "    # Hidden layers\n",
        "    layer_neurons = network_shape(params,1)\n",
        "\n",
        "    for i in range(params['hidden_layers']):\n",
        "\n",
        "        # Normalization layer\n",
        "        if params['batch_norm']==True:\n",
        "            model.add(BatchNormalization())\n",
        "\n",
        "        # Dense layer\n",
        "        model.add(Dense(layer_neurons[i],\n",
        "                        kernel_initializer=params['kernel_initializer'],\n",
        "                        kernel_regularizer = params['kernel_regularizer'],\n",
        "                        activity_regularizer = params['activity_regularizer'],\n",
        "                        use_bias=True))\n",
        "        \n",
        "        # Activation function  \n",
        "        if params['activation']=='prelu':\n",
        "            model.add( PReLU(alpha_initializer = Constant(value=params['alpha_initializer']) ))\n",
        "        elif params['activation']=='elu':\n",
        "            model.add(ELU(alpha=0.5))\n",
        "        elif params['activation']=='leaky_relu':\n",
        "            model.add(LeakyReLU())\n",
        "        else:\n",
        "            model.add(Activation(params['activation']))\n",
        "\n",
        "        # Dropout layer\n",
        "        if(params['dropout']>0):\n",
        "            model.add(Dropout(params['dropout']/2))\n",
        "\n",
        "    # Output Layer\n",
        "    model.add(Dense(1, kernel_initializer=params['kernel_initializer'], activation = 'sigmoid'))\n",
        "\n",
        "\n",
        "    # Optimizer\n",
        "    opt = optimizers.Adam(\n",
        "                learning_rate=lr_normalizer(params['lr'],Adam),\n",
        "                beta_1=0.9,\n",
        "                beta_2=0.999,\n",
        "                epsilon=1e-7,\n",
        "                amsgrad=False)\n",
        "    \n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt ,metrics=[metrics.AUC(curve='PR'),fbeta])#,\n",
        "    \n",
        "    return model\n",
        "\n",
        "def tox21_talos(x, y, x_v, y_v, params):\n",
        "\n",
        "    # Create model\n",
        "    model = tox21_model(x,y,params)\n",
        "\n",
        "    print(\"_____________________________________________________________________\")\n",
        "    for key in params:\n",
        "        print(f\"{key:20}{params[key]}\")\n",
        "\n",
        "    print(model.summary())\n",
        "\n",
        "    # logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "    es = EarlyStopping(monitor='val_loss', verbose=1, patience=25, restore_best_weights = True)\n",
        "    reduceLR=ReduceLROnPlateau(monitor='val_loss', \n",
        "                            factor=0.2, \n",
        "                            patience=10, \n",
        "                            verbose=1, \n",
        "                            min_delta=0, \n",
        "                            cooldown=0, min_lr=0)\n",
        "\n",
        "    history = model.fit(x, y, validation_data = (x_v, y_v),\n",
        "                        batch_size = params['batch_size'],\n",
        "                        epochs = params['epochs'],\n",
        "                        class_weight = params['class_weight'],\n",
        "                        callbacks=[es,reduceLR,prScore], #,TensorBoard(logdir, histogram_freq=1)\n",
        "                        verbose=2)\n",
        "    \n",
        "    return history, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo2bKRV6QfMM",
        "colab_type": "text"
      },
      "source": [
        "Hiperparametros\n",
        "\n",
        "Diccionario con los hiperparametros a evaluar. Debido a que la cantidad de hiperparametros es muy grande, se realizo una busqueda por partes para minimizar el tiempo de computo. Esta busqueda conistio en dejar fijos algunos hiperparametros y variar otros, siguiendo un orden:\n",
        "\n",
        "1. Funciones de activacion e inicializadores\n",
        "\n",
        "        Funciones de activacion: {Relu, Prelu, LeakyRelu, Helu, Tanh}\n",
        "\n",
        "        Inicializadores: {Random_uniform, Random_normal, He_uniform, He_normal, Glorot_normal}\n",
        "\n",
        "2. Neuronas, Hidden Layers, Shapes\n",
        "\n",
        "        Neuronas: {64, 256, 512, 1024}\n",
        "\n",
        "        Hidden Layers: {3,4,5}\n",
        "\n",
        "        Shapes: {Brick, Triangle, Funnel}\n",
        "\n",
        "3. Regularizacion, Dropout, Batch Normalization\n",
        "        \n",
        "        Regularizacion: {None, l1, l2}\n",
        "\n",
        "        Dropout: {0, 0.2, 0.4}\n",
        "\n",
        "        Batch Normalization: {True, False}\n",
        "4. Class Weight \n",
        "        class_weight: {None,{0:1.,1:2.},{0:1.,1:3.},{0:1.,1:5.},{0:1.,1:10.}}\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Y714J3nzs1n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p = {\n",
        "    'first_neuron': [512],                 # Tamaño de la primera capa\n",
        "    'dropout': [0,0.3],                    # Valor de dropout luego de la primera capa\n",
        "    'activation': ['relu','tanh'],                # Función de activación\n",
        "    'alpha_initializer': [0.2],             # PRELU alpha\n",
        "    'batch_size': [256],#, 512],                    # Tamaño de batch\n",
        "    'epochs': [400],                        # Number of epochs\n",
        "    'batch_norm': [True],                   # Batch normalization layers\n",
        "    'hidden_layers':[5],                    # Number of hidde layers\n",
        "    'shapes':['brick'],                     # Shape of hidden layers\n",
        "    'optimizer': [Adam],    # Optimizer\n",
        "    'class_weight': [None,{0:1.,1:2.},{0:1.,1:3.},{0:1.,1:5.},{0:1.,1:10.}],\n",
        "    'kernel_regularizer':[None,'l2'],            # Kernel (weights) regularization\n",
        "    #'bias_regularizer':[None],             # Bias (offsets) regularization\n",
        "    'activity_regularizer':[None],          # Activity (output) regularization\n",
        "    'kernel_initializer': ['random_normal','he_uniform'], # Kernel (weights) initializer\n",
        "    'lr': [0.1]                              # Learning rate\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrE3MuK2QjKm",
        "colab_type": "text"
      },
      "source": [
        "Talos Scan\n",
        "\n",
        "Se realiza el scan con cada grupo de hiperparametros y se guardan los resultados en distintos CSVs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoDAe1U1D4Z1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "%reload_ext tensorboard\n",
        "\n",
        "scan = talos.Scan (x=X_train,\n",
        "                    y=y_train,\n",
        "                    model=tox21_talos,\n",
        "                    experiment_name='tox21_SCAN_nr_er_class_weight2',\n",
        "                    #fraction_limit=0.004,\n",
        "                    #round_limit = 200,\n",
        "                    random_method = 'uniform_mersenne' ,\n",
        "                    x_val=X_val,\n",
        "                    y_val=y_val,\n",
        "                    params=p,\n",
        "                    print_params=False,) \n",
        "\n",
        "#El output de esta funcion se guarda en un archivo csv que permite visualizar los resultados de las metricas para cada combinacion de HP.\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyo9XGhyQnup",
        "colab_type": "text"
      },
      "source": [
        "## Modelo con mejores hiperparametros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCiVyN2AQ1aw",
        "colab_type": "text"
      },
      "source": [
        "Definicion del modelo\n",
        "\n",
        "Se define el modelo de acuerdo con los hiperparametros que dieron mejor resultado, los cuales se definieron inspeccionando los CSVs creados en la seccion anterior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9m7AqA05k3no",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "outputId": "9e1873d2-1fc2-4ef1-8e63-4b89e023f1f9"
      },
      "source": [
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(512, input_dim = X_train.shape[1], kernel_initializer='he_uniform', activation = 'relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512,kernel_regularizer = None, kernel_initializer='he_uniform', activation ='relu'))\n",
        "model.add(BatchNormalization())\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(512,kernel_regularizer = None,kernel_initializer='he_uniform', activation = 'relu'))\n",
        "model.add(BatchNormalization())\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(512,kernel_regularizer = None,kernel_initializer='he_uniform', activation = 'relu'))\n",
        "model.add(BatchNormalization())\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(512,kernel_regularizer = None,kernel_initializer='he_uniform', activation = 'relu'))\n",
        "model.add(BatchNormalization())\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(1,kernel_initializer='he_uniform', activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_25 (Dense)             (None, 512)               14336     \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "batch_normalization_24 (Batc (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 1,075,713\n",
            "Trainable params: 1,070,593\n",
            "Non-trainable params: 5,120\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1-V_CqFQ8UW",
        "colab_type": "text"
      },
      "source": [
        "Optimizador y compile\n",
        "\n",
        "Dado que las clases se encuentran desbalanceadas (88-12%), se decidio utilizar la metrica AUC, que es el area bajo la curva ROC. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6p6uD5gdLsBG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optimizers.Adam(\n",
        "                learning_rate=lr_normalizer(0.1,Adam),\n",
        "                beta_1=0.9,\n",
        "                beta_2=0.999,\n",
        "                epsilon=1e-7,\n",
        "                amsgrad=False)\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer=optimizer ,metrics=[AUC(curve='PR'),fbeta])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SytlMh2Q_x-",
        "colab_type": "text"
      },
      "source": [
        "Callbacks\n",
        "\n",
        "Se utilizara Early Stopping para detener el entrenamiento cuando la \"val_loss\" deje de mejorar, guardandose los pesos del mejor modelo. Tambien se utilizara el callback ReduceLROnPlateau para disminuir el Learning Rate cuando la \"val_loss\" no mejore."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA4J2b5SMHPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import Callback\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', verbose=1, patience=55, restore_best_weights = True)\n",
        "reduceLR=ReduceLROnPlateau(monitor='val_loss', \n",
        "                            factor=0.2, \n",
        "                            patience=10, \n",
        "                            verbose=1, \n",
        "                            min_delta=0, \n",
        "                            cooldown=0, min_lr=0)\n",
        "\n",
        "checkpoint_filepath = 'bestmodel.hdf5'\n",
        "model_checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_fbeta',\n",
        "    mode='max')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBk3vR5jRB-6",
        "colab_type": "text"
      },
      "source": [
        "Entrenamiento\n",
        "\n",
        "Se entrena el modelo planteado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I67QxBbilZ9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a6688af3-8552-43f7-a7b5-c6a2db19cfcb"
      },
      "source": [
        "hist = model.fit(X_train, y_train,\n",
        "                 batch_size=256, \n",
        "                 validation_data=(X_val, y_val),\n",
        "                 epochs=400, \n",
        "                 verbose=2,\n",
        "                 shuffle=True, \n",
        "                 callbacks=[model_checkpoint_callback, reduceLR,es],\n",
        "                 class_weight={0:1.,1:4})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 6927 samples, validate on 770 samples\n",
            "Epoch 1/400\n",
            " - 1s - loss: 0.9899 - auc_14: 0.2074 - fbeta: 0.4665 - val_loss: 0.6852 - val_auc_14: 0.2465 - val_fbeta: 0.3200\n",
            "Epoch 2/400\n",
            " - 0s - loss: 0.8780 - auc_14: 0.2718 - fbeta: 0.5347 - val_loss: 0.6186 - val_auc_14: 0.2873 - val_fbeta: 0.4411\n",
            "Epoch 3/400\n",
            " - 0s - loss: 0.8319 - auc_14: 0.3096 - fbeta: 0.5535 - val_loss: 0.5982 - val_auc_14: 0.3201 - val_fbeta: 0.5063\n",
            "Epoch 4/400\n",
            " - 0s - loss: 0.8017 - auc_14: 0.3295 - fbeta: 0.5583 - val_loss: 0.5748 - val_auc_14: 0.3367 - val_fbeta: 0.5779\n",
            "Epoch 5/400\n",
            " - 0s - loss: 0.7721 - auc_14: 0.3467 - fbeta: 0.5730 - val_loss: 0.5748 - val_auc_14: 0.3541 - val_fbeta: 0.6374\n",
            "Epoch 6/400\n",
            " - 0s - loss: 0.7415 - auc_14: 0.3633 - fbeta: 0.5807 - val_loss: 0.5655 - val_auc_14: 0.3702 - val_fbeta: 0.6543\n",
            "Epoch 7/400\n",
            " - 0s - loss: 0.7384 - auc_14: 0.3770 - fbeta: 0.5464 - val_loss: 0.5303 - val_auc_14: 0.3825 - val_fbeta: 0.6330\n",
            "Epoch 8/400\n",
            " - 0s - loss: 0.7118 - auc_14: 0.3880 - fbeta: 0.5954 - val_loss: 0.5314 - val_auc_14: 0.3926 - val_fbeta: 0.6374\n",
            "Epoch 9/400\n",
            " - 0s - loss: 0.6914 - auc_14: 0.3974 - fbeta: 0.6042 - val_loss: 0.5184 - val_auc_14: 0.4019 - val_fbeta: 0.6551\n",
            "Epoch 10/400\n",
            " - 0s - loss: 0.6774 - auc_14: 0.4066 - fbeta: 0.6013 - val_loss: 0.5260 - val_auc_14: 0.4110 - val_fbeta: 0.6468\n",
            "Epoch 11/400\n",
            " - 0s - loss: 0.6653 - auc_14: 0.4148 - fbeta: 0.5937 - val_loss: 0.5175 - val_auc_14: 0.4186 - val_fbeta: 0.6499\n",
            "Epoch 12/400\n",
            " - 0s - loss: 0.6678 - auc_14: 0.4210 - fbeta: 0.5816 - val_loss: 0.5141 - val_auc_14: 0.4231 - val_fbeta: 0.6450\n",
            "Epoch 13/400\n",
            " - 0s - loss: 0.6602 - auc_14: 0.4265 - fbeta: 0.5732 - val_loss: 0.4987 - val_auc_14: 0.4290 - val_fbeta: 0.6468\n",
            "Epoch 14/400\n",
            " - 0s - loss: 0.6402 - auc_14: 0.4319 - fbeta: 0.5743 - val_loss: 0.4903 - val_auc_14: 0.4344 - val_fbeta: 0.6452\n",
            "Epoch 15/400\n",
            " - 0s - loss: 0.6355 - auc_14: 0.4376 - fbeta: 0.6177 - val_loss: 0.4703 - val_auc_14: 0.4395 - val_fbeta: 0.6387\n",
            "Epoch 16/400\n",
            " - 0s - loss: 0.6283 - auc_14: 0.4429 - fbeta: 0.5741 - val_loss: 0.4531 - val_auc_14: 0.4448 - val_fbeta: 0.6339\n",
            "Epoch 17/400\n",
            " - 0s - loss: 0.6157 - auc_14: 0.4479 - fbeta: 0.5741 - val_loss: 0.4722 - val_auc_14: 0.4500 - val_fbeta: 0.6486\n",
            "Epoch 18/400\n",
            " - 0s - loss: 0.6171 - auc_14: 0.4530 - fbeta: 0.6033 - val_loss: 0.4743 - val_auc_14: 0.4549 - val_fbeta: 0.6260\n",
            "Epoch 19/400\n",
            " - 0s - loss: 0.6011 - auc_14: 0.4574 - fbeta: 0.6041 - val_loss: 0.4620 - val_auc_14: 0.4593 - val_fbeta: 0.6432\n",
            "Epoch 20/400\n",
            " - 0s - loss: 0.5999 - auc_14: 0.4615 - fbeta: 0.5726 - val_loss: 0.4529 - val_auc_14: 0.4635 - val_fbeta: 0.6307\n",
            "Epoch 21/400\n",
            " - 0s - loss: 0.5942 - auc_14: 0.4656 - fbeta: 0.5806 - val_loss: 0.4464 - val_auc_14: 0.4677 - val_fbeta: 0.6471\n",
            "Epoch 22/400\n",
            " - 0s - loss: 0.5965 - auc_14: 0.4696 - fbeta: 0.6030 - val_loss: 0.4425 - val_auc_14: 0.4707 - val_fbeta: 0.6287\n",
            "Epoch 23/400\n",
            " - 0s - loss: 0.5863 - auc_14: 0.4724 - fbeta: 0.5873 - val_loss: 0.4406 - val_auc_14: 0.4740 - val_fbeta: 0.6365\n",
            "Epoch 24/400\n",
            " - 0s - loss: 0.5844 - auc_14: 0.4755 - fbeta: 0.5816 - val_loss: 0.4324 - val_auc_14: 0.4772 - val_fbeta: 0.6315\n",
            "Epoch 25/400\n",
            " - 0s - loss: 0.5795 - auc_14: 0.4787 - fbeta: 0.5779 - val_loss: 0.4382 - val_auc_14: 0.4803 - val_fbeta: 0.6316\n",
            "Epoch 26/400\n",
            " - 0s - loss: 0.5661 - auc_14: 0.4820 - fbeta: 0.6165 - val_loss: 0.4464 - val_auc_14: 0.4832 - val_fbeta: 0.6306\n",
            "Epoch 27/400\n",
            " - 0s - loss: 0.5630 - auc_14: 0.4848 - fbeta: 0.5846 - val_loss: 0.4437 - val_auc_14: 0.4859 - val_fbeta: 0.6309\n",
            "Epoch 28/400\n",
            " - 0s - loss: 0.5605 - auc_14: 0.4878 - fbeta: 0.5798 - val_loss: 0.4381 - val_auc_14: 0.4888 - val_fbeta: 0.6228\n",
            "Epoch 29/400\n",
            " - 0s - loss: 0.5652 - auc_14: 0.4902 - fbeta: 0.5833 - val_loss: 0.4399 - val_auc_14: 0.4918 - val_fbeta: 0.6049\n",
            "Epoch 30/400\n",
            " - 0s - loss: 0.5565 - auc_14: 0.4931 - fbeta: 0.6263 - val_loss: 0.4257 - val_auc_14: 0.4945 - val_fbeta: 0.6292\n",
            "Epoch 31/400\n",
            " - 0s - loss: 0.5591 - auc_14: 0.4961 - fbeta: 0.6377 - val_loss: 0.4294 - val_auc_14: 0.4972 - val_fbeta: 0.6254\n",
            "Epoch 32/400\n",
            " - 0s - loss: 0.5480 - auc_14: 0.4988 - fbeta: 0.6183 - val_loss: 0.4359 - val_auc_14: 0.4999 - val_fbeta: 0.6117\n",
            "Epoch 33/400\n",
            " - 0s - loss: 0.5439 - auc_14: 0.5011 - fbeta: 0.6478 - val_loss: 0.4400 - val_auc_14: 0.5023 - val_fbeta: 0.6224\n",
            "Epoch 34/400\n",
            " - 0s - loss: 0.5441 - auc_14: 0.5040 - fbeta: 0.6036 - val_loss: 0.4295 - val_auc_14: 0.5051 - val_fbeta: 0.6228\n",
            "Epoch 35/400\n",
            " - 0s - loss: 0.5372 - auc_14: 0.5066 - fbeta: 0.6591 - val_loss: 0.4261 - val_auc_14: 0.5077 - val_fbeta: 0.6233\n",
            "Epoch 36/400\n",
            " - 0s - loss: 0.5445 - auc_14: 0.5091 - fbeta: 0.6281 - val_loss: 0.4309 - val_auc_14: 0.5098 - val_fbeta: 0.6368\n",
            "Epoch 37/400\n",
            " - 0s - loss: 0.5458 - auc_14: 0.5110 - fbeta: 0.6034 - val_loss: 0.4327 - val_auc_14: 0.5120 - val_fbeta: 0.6286\n",
            "Epoch 38/400\n",
            " - 0s - loss: 0.5330 - auc_14: 0.5132 - fbeta: 0.5973 - val_loss: 0.4281 - val_auc_14: 0.5141 - val_fbeta: 0.6103\n",
            "Epoch 39/400\n",
            " - 0s - loss: 0.5200 - auc_14: 0.5154 - fbeta: 0.6099 - val_loss: 0.4249 - val_auc_14: 0.5165 - val_fbeta: 0.6253\n",
            "Epoch 40/400\n",
            " - 0s - loss: 0.5275 - auc_14: 0.5176 - fbeta: 0.6174 - val_loss: 0.4307 - val_auc_14: 0.5186 - val_fbeta: 0.6384\n",
            "Epoch 41/400\n",
            " - 0s - loss: 0.5142 - auc_14: 0.5198 - fbeta: 0.6156 - val_loss: 0.4358 - val_auc_14: 0.5210 - val_fbeta: 0.6158\n",
            "Epoch 42/400\n",
            " - 0s - loss: 0.5283 - auc_14: 0.5221 - fbeta: 0.6517 - val_loss: 0.4348 - val_auc_14: 0.5230 - val_fbeta: 0.6222\n",
            "Epoch 43/400\n",
            " - 0s - loss: 0.5135 - auc_14: 0.5240 - fbeta: 0.6300 - val_loss: 0.4346 - val_auc_14: 0.5250 - val_fbeta: 0.6203\n",
            "Epoch 44/400\n",
            " - 0s - loss: 0.5098 - auc_14: 0.5262 - fbeta: 0.6391 - val_loss: 0.4348 - val_auc_14: 0.5273 - val_fbeta: 0.6031\n",
            "Epoch 45/400\n",
            " - 0s - loss: 0.5095 - auc_14: 0.5286 - fbeta: 0.6146 - val_loss: 0.4235 - val_auc_14: 0.5293 - val_fbeta: 0.6116\n",
            "Epoch 46/400\n",
            " - 0s - loss: 0.5048 - auc_14: 0.5305 - fbeta: 0.6396 - val_loss: 0.4346 - val_auc_14: 0.5313 - val_fbeta: 0.6035\n",
            "Epoch 47/400\n",
            " - 0s - loss: 0.5004 - auc_14: 0.5324 - fbeta: 0.6723 - val_loss: 0.4395 - val_auc_14: 0.5332 - val_fbeta: 0.6286\n",
            "Epoch 48/400\n",
            " - 0s - loss: 0.4884 - auc_14: 0.5345 - fbeta: 0.6464 - val_loss: 0.4429 - val_auc_14: 0.5355 - val_fbeta: 0.6077\n",
            "Epoch 49/400\n",
            " - 0s - loss: 0.4932 - auc_14: 0.5367 - fbeta: 0.6631 - val_loss: 0.4370 - val_auc_14: 0.5375 - val_fbeta: 0.6310\n",
            "Epoch 50/400\n",
            " - 0s - loss: 0.5005 - auc_14: 0.5385 - fbeta: 0.6710 - val_loss: 0.4298 - val_auc_14: 0.5392 - val_fbeta: 0.6091\n",
            "Epoch 51/400\n",
            " - 0s - loss: 0.4874 - auc_14: 0.5407 - fbeta: 0.6865 - val_loss: 0.4326 - val_auc_14: 0.5414 - val_fbeta: 0.5809\n",
            "Epoch 52/400\n",
            " - 0s - loss: 0.4876 - auc_14: 0.5427 - fbeta: 0.6760 - val_loss: 0.4255 - val_auc_14: 0.5434 - val_fbeta: 0.5900\n",
            "Epoch 53/400\n",
            " - 0s - loss: 0.4847 - auc_14: 0.5446 - fbeta: 0.6559 - val_loss: 0.4271 - val_auc_14: 0.5454 - val_fbeta: 0.6125\n",
            "Epoch 54/400\n",
            " - 0s - loss: 0.4937 - auc_14: 0.5463 - fbeta: 0.6291 - val_loss: 0.4405 - val_auc_14: 0.5469 - val_fbeta: 0.6307\n",
            "Epoch 55/400\n",
            " - 0s - loss: 0.4849 - auc_14: 0.5478 - fbeta: 0.6318 - val_loss: 0.4450 - val_auc_14: 0.5484 - val_fbeta: 0.6284\n",
            "\n",
            "Epoch 00055: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
            "Epoch 56/400\n",
            " - 0s - loss: 0.4710 - auc_14: 0.5498 - fbeta: 0.7047 - val_loss: 0.4421 - val_auc_14: 0.5506 - val_fbeta: 0.6247\n",
            "Epoch 57/400\n",
            " - 0s - loss: 0.4523 - auc_14: 0.5518 - fbeta: 0.7094 - val_loss: 0.4404 - val_auc_14: 0.5526 - val_fbeta: 0.6236\n",
            "Epoch 58/400\n",
            " - 0s - loss: 0.4582 - auc_14: 0.5536 - fbeta: 0.7038 - val_loss: 0.4438 - val_auc_14: 0.5544 - val_fbeta: 0.6276\n",
            "Epoch 59/400\n",
            " - 0s - loss: 0.4606 - auc_14: 0.5554 - fbeta: 0.6572 - val_loss: 0.4379 - val_auc_14: 0.5562 - val_fbeta: 0.6226\n",
            "Epoch 60/400\n",
            " - 0s - loss: 0.4596 - auc_14: 0.5573 - fbeta: 0.6762 - val_loss: 0.4391 - val_auc_14: 0.5581 - val_fbeta: 0.6306\n",
            "Epoch 61/400\n",
            " - 0s - loss: 0.4660 - auc_14: 0.5590 - fbeta: 0.6843 - val_loss: 0.4340 - val_auc_14: 0.5598 - val_fbeta: 0.6277\n",
            "Epoch 62/400\n",
            " - 0s - loss: 0.4686 - auc_14: 0.5608 - fbeta: 0.6989 - val_loss: 0.4305 - val_auc_14: 0.5614 - val_fbeta: 0.6331\n",
            "Epoch 63/400\n",
            " - 0s - loss: 0.4656 - auc_14: 0.5623 - fbeta: 0.6823 - val_loss: 0.4339 - val_auc_14: 0.5629 - val_fbeta: 0.6245\n",
            "Epoch 64/400\n",
            " - 0s - loss: 0.4527 - auc_14: 0.5639 - fbeta: 0.6716 - val_loss: 0.4352 - val_auc_14: 0.5646 - val_fbeta: 0.6316\n",
            "Epoch 65/400\n",
            " - 0s - loss: 0.4643 - auc_14: 0.5653 - fbeta: 0.6901 - val_loss: 0.4376 - val_auc_14: 0.5660 - val_fbeta: 0.6265\n",
            "\n",
            "Epoch 00065: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
            "Epoch 66/400\n",
            " - 0s - loss: 0.4564 - auc_14: 0.5668 - fbeta: 0.7149 - val_loss: 0.4354 - val_auc_14: 0.5675 - val_fbeta: 0.6274\n",
            "Epoch 67/400\n",
            " - 0s - loss: 0.4441 - auc_14: 0.5685 - fbeta: 0.7129 - val_loss: 0.4349 - val_auc_14: 0.5692 - val_fbeta: 0.6274\n",
            "Epoch 68/400\n",
            " - 0s - loss: 0.4506 - auc_14: 0.5701 - fbeta: 0.6895 - val_loss: 0.4332 - val_auc_14: 0.5708 - val_fbeta: 0.6213\n",
            "Epoch 69/400\n",
            " - 0s - loss: 0.4557 - auc_14: 0.5717 - fbeta: 0.6743 - val_loss: 0.4334 - val_auc_14: 0.5722 - val_fbeta: 0.6282\n",
            "Epoch 70/400\n",
            " - 0s - loss: 0.4652 - auc_14: 0.5729 - fbeta: 0.6995 - val_loss: 0.4341 - val_auc_14: 0.5734 - val_fbeta: 0.6244\n",
            "Epoch 71/400\n",
            " - 0s - loss: 0.4588 - auc_14: 0.5741 - fbeta: 0.6667 - val_loss: 0.4354 - val_auc_14: 0.5747 - val_fbeta: 0.6161\n",
            "Epoch 72/400\n",
            " - 0s - loss: 0.4487 - auc_14: 0.5755 - fbeta: 0.6790 - val_loss: 0.4374 - val_auc_14: 0.5761 - val_fbeta: 0.6161\n",
            "Epoch 73/400\n",
            " - 0s - loss: 0.4579 - auc_14: 0.5768 - fbeta: 0.6811 - val_loss: 0.4357 - val_auc_14: 0.5773 - val_fbeta: 0.6235\n",
            "Epoch 74/400\n",
            " - 0s - loss: 0.4514 - auc_14: 0.5782 - fbeta: 0.7092 - val_loss: 0.4344 - val_auc_14: 0.5786 - val_fbeta: 0.6244\n",
            "Epoch 75/400\n",
            " - 0s - loss: 0.4530 - auc_14: 0.5793 - fbeta: 0.7159 - val_loss: 0.4340 - val_auc_14: 0.5798 - val_fbeta: 0.6309\n",
            "\n",
            "Epoch 00075: ReduceLROnPlateau reducing learning rate to 7.999999979801942e-07.\n",
            "Epoch 76/400\n",
            " - 0s - loss: 0.4549 - auc_14: 0.5806 - fbeta: 0.6867 - val_loss: 0.4347 - val_auc_14: 0.5811 - val_fbeta: 0.6309\n",
            "Epoch 77/400\n",
            " - 0s - loss: 0.4503 - auc_14: 0.5817 - fbeta: 0.7060 - val_loss: 0.4340 - val_auc_14: 0.5822 - val_fbeta: 0.6309\n",
            "Epoch 78/400\n",
            " - 0s - loss: 0.4484 - auc_14: 0.5830 - fbeta: 0.6815 - val_loss: 0.4355 - val_auc_14: 0.5835 - val_fbeta: 0.6309\n",
            "Epoch 79/400\n",
            " - 0s - loss: 0.4598 - auc_14: 0.5840 - fbeta: 0.6761 - val_loss: 0.4349 - val_auc_14: 0.5845 - val_fbeta: 0.6272\n",
            "Epoch 80/400\n",
            " - 0s - loss: 0.4527 - auc_14: 0.5851 - fbeta: 0.7031 - val_loss: 0.4343 - val_auc_14: 0.5856 - val_fbeta: 0.6309\n",
            "Epoch 81/400\n",
            " - 0s - loss: 0.4566 - auc_14: 0.5862 - fbeta: 0.6729 - val_loss: 0.4352 - val_auc_14: 0.5867 - val_fbeta: 0.6309\n",
            "Epoch 82/400\n",
            " - 0s - loss: 0.4485 - auc_14: 0.5874 - fbeta: 0.6795 - val_loss: 0.4351 - val_auc_14: 0.5879 - val_fbeta: 0.6319\n",
            "Epoch 83/400\n",
            " - 0s - loss: 0.4581 - auc_14: 0.5885 - fbeta: 0.6810 - val_loss: 0.4343 - val_auc_14: 0.5888 - val_fbeta: 0.6319\n",
            "Epoch 84/400\n",
            " - 0s - loss: 0.4536 - auc_14: 0.5895 - fbeta: 0.6903 - val_loss: 0.4328 - val_auc_14: 0.5899 - val_fbeta: 0.6319\n",
            "Epoch 85/400\n",
            " - 0s - loss: 0.4473 - auc_14: 0.5905 - fbeta: 0.6869 - val_loss: 0.4341 - val_auc_14: 0.5910 - val_fbeta: 0.6282\n",
            "\n",
            "Epoch 00085: ReduceLROnPlateau reducing learning rate to 1.600000018697756e-07.\n",
            "Epoch 86/400\n",
            " - 0s - loss: 0.4533 - auc_14: 0.5917 - fbeta: 0.6943 - val_loss: 0.4339 - val_auc_14: 0.5920 - val_fbeta: 0.6273\n",
            "Epoch 87/400\n",
            " - 0s - loss: 0.4582 - auc_14: 0.5924 - fbeta: 0.6937 - val_loss: 0.4343 - val_auc_14: 0.5928 - val_fbeta: 0.6319\n",
            "Epoch 88/400\n",
            " - 0s - loss: 0.4556 - auc_14: 0.5935 - fbeta: 0.6792 - val_loss: 0.4358 - val_auc_14: 0.5937 - val_fbeta: 0.6319\n",
            "Epoch 89/400\n",
            " - 0s - loss: 0.4435 - auc_14: 0.5943 - fbeta: 0.6967 - val_loss: 0.4354 - val_auc_14: 0.5947 - val_fbeta: 0.6319\n",
            "Epoch 90/400\n",
            " - 0s - loss: 0.4437 - auc_14: 0.5954 - fbeta: 0.6885 - val_loss: 0.4362 - val_auc_14: 0.5957 - val_fbeta: 0.6319\n",
            "Epoch 91/400\n",
            " - 0s - loss: 0.4438 - auc_14: 0.5962 - fbeta: 0.7104 - val_loss: 0.4353 - val_auc_14: 0.5966 - val_fbeta: 0.6319\n",
            "Epoch 92/400\n",
            " - 0s - loss: 0.4461 - auc_14: 0.5973 - fbeta: 0.6875 - val_loss: 0.4355 - val_auc_14: 0.5976 - val_fbeta: 0.6282\n",
            "Epoch 93/400\n",
            " - 0s - loss: 0.4500 - auc_14: 0.5981 - fbeta: 0.7254 - val_loss: 0.4364 - val_auc_14: 0.5985 - val_fbeta: 0.6310\n",
            "Epoch 94/400\n",
            " - 0s - loss: 0.4529 - auc_14: 0.5990 - fbeta: 0.7031 - val_loss: 0.4366 - val_auc_14: 0.5993 - val_fbeta: 0.6309\n",
            "Epoch 95/400\n",
            " - 0s - loss: 0.4545 - auc_14: 0.5998 - fbeta: 0.6879 - val_loss: 0.4360 - val_auc_14: 0.6001 - val_fbeta: 0.6310\n",
            "\n",
            "Epoch 00095: ReduceLROnPlateau reducing learning rate to 3.199999980552093e-08.\n",
            "Epoch 96/400\n",
            " - 0s - loss: 0.4477 - auc_14: 0.6006 - fbeta: 0.7230 - val_loss: 0.4360 - val_auc_14: 0.6010 - val_fbeta: 0.6319\n",
            "Epoch 97/400\n",
            " - 0s - loss: 0.4400 - auc_14: 0.6015 - fbeta: 0.6973 - val_loss: 0.4372 - val_auc_14: 0.6018 - val_fbeta: 0.6302\n",
            "Epoch 98/400\n",
            " - 0s - loss: 0.4520 - auc_14: 0.6023 - fbeta: 0.6883 - val_loss: 0.4373 - val_auc_14: 0.6025 - val_fbeta: 0.6292\n",
            "Epoch 99/400\n",
            " - 0s - loss: 0.4460 - auc_14: 0.6030 - fbeta: 0.6987 - val_loss: 0.4365 - val_auc_14: 0.6034 - val_fbeta: 0.6300\n",
            "Epoch 100/400\n",
            " - 0s - loss: 0.4568 - auc_14: 0.6038 - fbeta: 0.6716 - val_loss: 0.4370 - val_auc_14: 0.6041 - val_fbeta: 0.6309\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00100: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2nnK1B4dOcy",
        "colab_type": "text"
      },
      "source": [
        "Carga los pesos del modelo con la mejor performance de fbeta_score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PreCoRWNXhEW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights(checkpoint_filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7g0BvRKTi5Y",
        "colab_type": "text"
      },
      "source": [
        "# Dataset de TEST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia5Q_9qNpoR0",
        "colab_type": "text"
      },
      "source": [
        "Archivo con los descriptores del dataset de test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBOvEnxJo7TT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "outputId": "b68d37ef-7859-4a3b-ab16-9ae326da8a9a"
      },
      "source": [
        "desc = 'Descriptores_test_new.csv' #Este archivo se obtuvo con la notebook tox21_DescriptorsCalc\n",
        "descriptores_test = pd.read_csv(desc)\n",
        "descriptores_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>abonds</th>\n",
              "      <th>atoms</th>\n",
              "      <th>bonds</th>\n",
              "      <th>cansmi</th>\n",
              "      <th>cansmiNS</th>\n",
              "      <th>dbonds</th>\n",
              "      <th>formula</th>\n",
              "      <th>HBA1</th>\n",
              "      <th>HBA2</th>\n",
              "      <th>HBD</th>\n",
              "      <th>InChI</th>\n",
              "      <th>InChIKey</th>\n",
              "      <th>L5</th>\n",
              "      <th>logP</th>\n",
              "      <th>MP</th>\n",
              "      <th>MR</th>\n",
              "      <th>MW</th>\n",
              "      <th>nF</th>\n",
              "      <th>rotors</th>\n",
              "      <th>s</th>\n",
              "      <th>sbonds</th>\n",
              "      <th>smarts</th>\n",
              "      <th>tbonds</th>\n",
              "      <th>title</th>\n",
              "      <th>TPSA</th>\n",
              "      <th>CHARGE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>102.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.3452</td>\n",
              "      <td>323.4901</td>\n",
              "      <td>193.7180</td>\n",
              "      <td>610.831900</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>91.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>72.88</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>10.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.9645</td>\n",
              "      <td>234.5760</td>\n",
              "      <td>76.3992</td>\n",
              "      <td>407.094061</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>167.48</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>21.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.1009</td>\n",
              "      <td>260.0716</td>\n",
              "      <td>87.4097</td>\n",
              "      <td>287.315320</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.68</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>17.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.4267</td>\n",
              "      <td>127.8196</td>\n",
              "      <td>87.9497</td>\n",
              "      <td>328.355826</td>\n",
              "      <td>2.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>37.91</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>12.0</td>\n",
              "      <td>41.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.1642</td>\n",
              "      <td>121.6301</td>\n",
              "      <td>91.2077</td>\n",
              "      <td>351.847720</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71.62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>642</th>\n",
              "      <td>642</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2986</td>\n",
              "      <td>14.3068</td>\n",
              "      <td>25.6258</td>\n",
              "      <td>92.160020</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>59.03</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>643</th>\n",
              "      <td>643</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.4309</td>\n",
              "      <td>57.7851</td>\n",
              "      <td>67.1814</td>\n",
              "      <td>219.281340</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>70.41</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>644</th>\n",
              "      <td>644</td>\n",
              "      <td>5.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.1273</td>\n",
              "      <td>126.9533</td>\n",
              "      <td>26.4687</td>\n",
              "      <td>116.144960</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>78.59</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>645</th>\n",
              "      <td>645</td>\n",
              "      <td>11.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.3393</td>\n",
              "      <td>221.4786</td>\n",
              "      <td>86.1255</td>\n",
              "      <td>344.259400</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>85.93</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>646</th>\n",
              "      <td>646</td>\n",
              "      <td>5.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.7094</td>\n",
              "      <td>26.6394</td>\n",
              "      <td>31.6030</td>\n",
              "      <td>114.165540</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>51.94</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>647 rows × 27 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Unnamed: 0  abonds  atoms  bonds  ...  tbonds  title    TPSA  CHARGE\n",
              "0             0     6.0   95.0  102.0  ...     0.0    0.0   72.88       0\n",
              "1             1    10.0   33.0   36.0  ...     0.0    0.0  167.48      -1\n",
              "2             2    21.0   35.0   39.0  ...     0.0    0.0   50.68       0\n",
              "3             3    17.0   42.0   44.0  ...     0.0    0.0   37.91       0\n",
              "4             4    12.0   41.0   42.0  ...     0.0    0.0   71.62       0\n",
              "..          ...     ...    ...    ...  ...     ...    ...     ...     ...\n",
              "642         642     0.0   13.0   12.0  ...     0.0    0.0   59.03       0\n",
              "643         643     0.0   36.0   36.0  ...     0.0    0.0   70.41       0\n",
              "644         644     5.0   11.0   11.0  ...     0.0    0.0   78.59       0\n",
              "645         645    11.0   36.0   38.0  ...     0.0    0.0   85.93       0\n",
              "646         646     5.0   13.0   13.0  ...     0.0    0.0   51.94       0\n",
              "\n",
              "[647 rows x 27 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnTZUgZ_pw8V",
        "colab_type": "text"
      },
      "source": [
        "Obtenemos el vector de clases segun las actividades\n",
        "\n",
        "0 : inactivo\n",
        "\n",
        "1 : activo\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd_GI44dTwMf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e4198da2-7c3e-4ba9-e45e-4e574b37ccc8"
      },
      "source": [
        "#Read test targets.\n",
        "actives_f = open(\"nr_er_test_new_activity.txt\", \"r\") #Este archivo se obtuvo de la columna de Actividad del SDF.\n",
        "test_activity = []\n",
        "for i,x in enumerate(actives_f):\n",
        "  if(x[0] != 'x'):\n",
        "    test_activity.append(x[0])\n",
        "  else:\n",
        "    descriptores_test = descriptores_test.drop(i,axis =0)\n",
        "test_activity = [int(i) for i in test_activity]\n",
        "y_test = np.array(test_activity)\n",
        "print(y_test.shape)\n",
        "actives_f.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(516,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHMrsRCrp6f5",
        "colab_type": "text"
      },
      "source": [
        "Corroboramos la distribucion de clases en test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS8VkvC_UB2E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "e4caa0fd-41d1-4a9f-b51a-a790caf585b0"
      },
      "source": [
        "#Visualizar activos vs inactivos en Test: \n",
        "activos = sum(y_test)\n",
        "inactivos = sum(y_test == 0)\n",
        "print('Proporcion de activos en el dataset:', activos/(activos+inactivos))\n",
        "indices = ['Activos','Inactivos']\n",
        "activity_df = pd.DataFrame(data = [activos, inactivos], columns = ['Test'], index = indices)\n",
        "activity_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Proporcion de activos en el dataset: 0.09883720930232558\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Activos</th>\n",
              "      <td>51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Inactivos</th>\n",
              "      <td>465</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Test\n",
              "Activos      51\n",
              "Inactivos   465"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IM4fdn2dqSQd",
        "colab_type": "text"
      },
      "source": [
        "Normalizacion de los datos de Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SN9k3VAFUVzN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "36b7d859-8deb-48de-fee2-67f4992371e9"
      },
      "source": [
        "#Test Dataset Scale:\n",
        "X_test = desc_scaler.transform(descriptores_test)\n",
        "X_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(516, 27)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lHsf4I_qW9F",
        "colab_type": "text"
      },
      "source": [
        "Evaluacion de Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFNMWqwNbiYr",
        "colab_type": "text"
      },
      "source": [
        "Funcion de evaluacion para hallar el umbral que resulta en el mejor fbeta_score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY9txzJIbf_a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "# from sklearn.metrics import auc, roc_auc_score, accuracy_score, roc_curve\n",
        "from sklearn.metrics import fbeta_score\n",
        "\n",
        "def best_fbeta(y_true,y_pred,plot=False,beta=2):\n",
        "    thresholds = np.arange(100)/100\n",
        "    scores = np.zeros(thresholds.shape)\n",
        "\n",
        "\n",
        "    for i, thr in enumerate(thresholds.tolist()):\n",
        "        y_pred_class = (y_pred>thr)*1\n",
        "        scores[i] = fbeta_score(y_true,y_pred_class,beta=beta)\n",
        "\n",
        "    best_score = np.max(scores)\n",
        "    best_score_idx = np.argmax(scores)\n",
        "    best_thr = thresholds[best_score_idx]\n",
        "\n",
        "    if plot:\n",
        "        plt.plot(thresholds,scores)  \n",
        "        plt.axvline(x=best_thr,color='r')\n",
        "        plt.xlabel('threshold')\n",
        "        plt.ylabel('fbeta_score')\n",
        "    return [best_thr,best_score]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD_5GLRJeB1Q",
        "colab_type": "text"
      },
      "source": [
        "Evaluacion de test AUC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OiV3E0yUpOS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f46efa82-f68f-48d2-f2c4-411e1f19a0d1"
      },
      "source": [
        "#Predict for Test:\n",
        "test_pred = model.predict_proba(X_test)\n",
        "#Evaluar metrica AUC en test\n",
        "m = metrics.AUC()\n",
        "m.update_state(y_test,test_pred)\n",
        "auc_test = m.result().numpy()\n",
        "\n",
        "print(auc_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.733481\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qVYMNwcxuJH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "outputId": "cdf4d887-8729-432f-f047-a2bd485cfac9"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "beta=2\n",
        "val_pred = model.predict_proba(X_val)\n",
        "best_thr, best_score = best_fbeta(y_val,val_pred,beta=beta,plot=True)\n",
        "print('El mejor f2Score es de {:.3f} y se obtiene tomando el umbral {}'.format(best_score,best_thr))\n",
        "print(confusion_matrix(y_val,(val_pred>best_thr)*1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "El mejor f2Score es de 0.518 y se obtiene tomando el umbral 0.52\n",
            "[[602  74]\n",
            " [ 42  52]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leVmM9o_eOO9",
        "colab_type": "text"
      },
      "source": [
        "Busqueda del umbral para el mejor fbeta_score. Se elije beta=2 para darle mas importancia al Recall (sensibilidad) ya que es mas importante clasificar bien la clase positiva (compuestos toxicos)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPNQ8zXJPiPd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "88404737-7a6c-4b1c-ea93-8e9f8b938ece"
      },
      "source": [
        "test_pred = model.predict_proba(X_test)\n",
        "fscore_test = fbeta_score(y_test,(test_pred>best_thr)*1,beta=beta)\n",
        "print('El f2Score para test es de {:.3f}'.format(fscore_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "El f2Score para test es de 0.496\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBitSqGBfzrj",
        "colab_type": "text"
      },
      "source": [
        "Con el umbral hallado, se calcula la matriz de confusion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTSEmKTVQmKR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5182e04f-102f-4aa9-e2ae-58eea6e8f01f"
      },
      "source": [
        "print(confusion_matrix(y_test,(test_pred>best_thr)*1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[342 123]\n",
            " [ 15  36]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m91voZB8sKs4",
        "colab_type": "text"
      },
      "source": [
        "# Discusión y conclusiones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08vsstxasWdU",
        "colab_type": "text"
      },
      "source": [
        "Para evaluar el roc_auc score obtenido para el dataset de test se lo compara con el del equipo ganador de la competencia Tox21 Data Challenge, el unico equipo en utilizar redes neuronales. Nuestro resultado es comparable con el resultado obtenido por el equipo ganador que fue de 0.810. \n",
        "\n",
        "Creemos que la principal razón de la diferencia entre los resultados puede deberse a la cantidad de descriptores utilizados para la clasificación. En nuestro caso contamos únicamente con 27 descriptores mientras que en el trabajo del equipo ganador se utilizaron mas de mil descriptores. También puede deberse a la elección de los hiperparámetros.\n",
        "\n",
        "Dado que se resulta mas importante clasificar bien la clase positiva (compuestos toxicos), se utilizo la metrica fBeta_score. La misma permite darle mas importancia a la sensibilidad del modelo si se elige Beta mayor a 1. En este caso se utilizo Beta igual a 2 ya que producia una relacion de compromiso aceptable entre los VP,VN,FN y FP. Para el set de validacion se utilizo la funcion best_fbeta para hallar el umbral que mejor score produciera. Con este umbral se realizo la clasificacion para el set de test y se realizo la matriz de confusion, en la que se puede ver una mejor clasificacion de la clase positiva que en la entrega previa.\n",
        " \n"
      ]
    }
  ]
}